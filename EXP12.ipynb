{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "EXP12.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Anakha-s/Data-Science/blob/main/EXP12.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "WEB SCRAPPING"
      ],
      "metadata": {
        "id": "b6eY-uSln8Cx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install requests"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JG_CMkMLmJHx",
        "outputId": "96bd04fa-678d-4f52-ce60-3bc19fd6b8cf"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (2.23.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests) (2021.10.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests) (2.10)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Requests making GET request"
      ],
      "metadata": {
        "id": "z2n8f1_8oHFe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import requests\n",
        "  \n",
        "# Making a GET request\n",
        "r = requests.get('https://www.geeksforgeeks.org/python-programming-language/')\n",
        "  \n",
        "# check status code for response received\n",
        "# success code - 200\n",
        "print(r)\n",
        "  \n",
        "# print content of request\n",
        "print(r.content)"
      ],
      "metadata": {
        "id": "J2gy-01umbtC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "install Beautifulsoup"
      ],
      "metadata": {
        "id": "0w655PXWn6nU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install beautifulsoup4"
      ],
      "metadata": {
        "id": "_23UKIZwmyQp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "BeautifulSoup Parsing HTML"
      ],
      "metadata": {
        "id": "4KANMPuOoUe9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "  \n",
        "  \n",
        "# Making a GET request\n",
        "r = requests.get('https://www.geeksforgeeks.org/python-programming-language/')\n",
        "  \n",
        "# check status code for response received\n",
        "# success code - 200\n",
        "print(r)\n",
        "  \n",
        "# Parsing the HTML\n",
        "soup = BeautifulSoup(r.content, 'html.parser')\n",
        "print(soup.prettify())"
      ],
      "metadata": {
        "id": "8Z-Dza5qmzih"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "finding all the p tags present"
      ],
      "metadata": {
        "id": "ou3tCWUxokKK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "  \n",
        "  \n",
        "# Making a GET request\n",
        "r = requests.get('https://www.geeksforgeeks.org/python-programming-language/')\n",
        "  \n",
        "# Parsing the HTML\n",
        "soup = BeautifulSoup(r.content, 'html.parser')\n",
        "  \n",
        "s = soup.find('div', class_='entry-content')\n",
        "content = s.find_all('p')\n",
        "  \n",
        "print(content)"
      ],
      "metadata": {
        "id": "GmbNVb2HnIjC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Removing the tags from the content of the page "
      ],
      "metadata": {
        "id": "n_eKIv3apFeL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "  \n",
        "  \n",
        "# Making a GET request\n",
        "r = requests.get('https://www.geeksforgeeks.org/python-programming-language/')\n",
        "  \n",
        "# Parsing the HTML\n",
        "soup = BeautifulSoup(r.content, 'html.parser')\n",
        "  \n",
        "s = soup.find('div', class_='entry-content')\n",
        "  \n",
        "lines = s.find_all('p')\n",
        "  \n",
        "for line in lines:\n",
        "    print(line.text)"
      ],
      "metadata": {
        "id": "Vcyb-EG5nWrx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " Python BeautifulSoup Extracting Links"
      ],
      "metadata": {
        "id": "vs2Q7ASPpLAi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "  \n",
        "  \n",
        "# Making a GET request\n",
        "r = requests.get('https://www.geeksforgeeks.org/python-programming-language/')\n",
        "  \n",
        "# Parsing the HTML\n",
        "soup = BeautifulSoup(r.content, 'html.parser')\n",
        "  \n",
        "# find all the anchor tags with \"href\" \n",
        "for link in soup.find_all('a'):\n",
        "    print(link.get('href'))"
      ],
      "metadata": {
        "id": "k5jUxv_ZnbtD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Looping through the page numbers"
      ],
      "metadata": {
        "id": "EZ1nU4pZu76c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup as bs\n",
        "  \n",
        "URL = 'https://www.geeksforgeeks.org/page/1/'\n",
        "  \n",
        "req = requests.get(URL)\n",
        "soup = bs(req.text, 'html.parser')\n",
        "  \n",
        "titles = soup.find_all('div',attrs = {'class','head'})\n",
        "  \n",
        "print(titles[4].text)"
      ],
      "metadata": {
        "id": "rIVRl-BfrNYb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Python BeautifulSoup saving to CSV"
      ],
      "metadata": {
        "id": "MSP0dtXsvLHT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup as bs\n",
        "import csv\n",
        "  \n",
        "URL = 'https://www.geeksforgeeks.org/page/'\n",
        "  \n",
        "soup = bs(req.text, 'html.parser')\n",
        "  \n",
        "titles = soup.find_all('div', attrs={'class', 'head'})\n",
        "titles_list = []\n",
        "  \n",
        "count = 1\n",
        "for title in titles:\n",
        "    d = {}\n",
        "    d['Title Number'] = f'Title {count}'\n",
        "    d['Title Name'] = title.text\n",
        "    count += 1\n",
        "    titles_list.append(d)\n",
        "  \n",
        "filename = 'titles.csv'\n",
        "with open(filename, 'w', newline='') as f:\n",
        "    w = csv.DictWriter(f,['Title Number','Title Name'])\n",
        "    w.writeheader()\n",
        "      \n",
        "    w.writerows(titles_list)"
      ],
      "metadata": {
        "id": "634FKGouqcvr"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}